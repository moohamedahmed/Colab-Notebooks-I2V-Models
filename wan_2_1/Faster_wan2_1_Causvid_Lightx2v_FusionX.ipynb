{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Faster WAN 2.1 IMAGE TO VIDEO WITH CAUSVID, LIGHTX2V, & FUSION-X LoRAs**\n",
        "- You can use this notebook for basic image to video generation using the Wan2.1 LoRAs listed in the title. For a notebook that also does video upscale, enhancement, face correction/swap, and frame interploation, visit this link: https://isinse.gumroad.com/l/wan2point1withLoRAs\n",
        "- To read a guide on using this notebook, see this link: https://penioj.blogspot.com/2025/07/i-dumped-kling-ai-for-these-colab.html  \n",
        "- If you deselect the `use480p` checkbox, the 720p Wan 2.1 base model will be downloaded and used rather than the 480p Wan 2.1 base model. When used with the `walking to viewers` LoRA, the 720p model is about 3 minutes faster than the 480p model for high definition (720p) videos.\n",
        "- You can use the free T4 GPU to generate a 5 second 480P video (frames=81) at 16fps with the Q5_K_M GGUF 480p model and with the default settings in less than 10 minutes. A 4-second 720p video (frames=65) can be generated in roughly 22 minutes with the Q4_K_M 480p model, and in 19 minutes with the Q4_K_M 720p model. I recommend that you use higher GPUs for bigger models, longer videos, and faster generations.\n",
        "- **To use a lora, put its huggingface or civitai download link in the `lora_download_url` textbox, select the `download_lora` checkbox, and if using civitai, input your civitai token before running the code to `Prepare Environment`. Remember to describe the main subject of the image and include the trigger words for the LoRA in the prompt. For the default walking forward lora link in lora_2_download_url, the trigger word is 'walking to viewers.' You can get LoRAs from this huggingface repository: https://huggingface.co/collections/Remade-AI/wan21-14b-480p-i2v-loras-67d0e26f08092436b585919b and from civitai: https://civitai.com/models. In civitai, set the `Wan Video` and `LoRA` filters to see the Wan LoRAs.** You can watch this video to learn how to get and use LoRAs from huggingface and civitai, and how to create your civitai token: https://youtu.be/49NkIV_QpBM\n",
        "- Generating a video from this flux image (https://comfyanonymous.github.io/ComfyUI_examples/flux/) with the settings (480x480, 20 steps, 65 frames) using the Q4 GGUF model and the free T4 GPU took about 33 minutes with no Teacache i.e. `rel_l1_threshless` set to zero in the Teacache settings, and less than 18 minutes with `rel_l1_threshless` set to 0.275 with little loss in quality. Increase the value of `rel_l1_threshless` for faster generation with a tradeoff in quality. To get much faster generations, use the causvid, lightx2v or fusionx model LoRAs. It is recommended that you set `rel_l1_threshless` to zero if using these LoRAs.\n",
        "- **causvid recommended settings** : cfg_scale=1 , steps=4 ,sampler_name=uni_pc , sceduler=simple , flow_shift=5 , strength=0.8\n",
        "- **lightx2v recommended settings** : cfg_scale=1 , steps=4 ,sampler_name=LCM , sceduler=simple , flow_shift=8, strength=1\n",
        "- **fusionx recommended settings** : cfg_scale=1 , steps=6 ,sampler_name=uni_pc , sceduler=simple , flow_shift=5 , strength=1\n",
        "- You can enable both lightx2v & fusionx and adjust their strengths until you get a desirable result. fusionx already contains the causvid LoRA, but you can experiment with different combinations.\n"
      ],
      "metadata": {
        "id": "D2lupAO0HdyH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "96675e45HYsu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @markdown # ðŸ’¥1. Prepare Environment\n",
        "# !pip install --upgrade --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch==2.6.0 torchvision==0.21.0\n",
        "%cd /content\n",
        "from IPython.display import clear_output\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.29.post2 triton==3.2.0 sageattention\n",
        "!pip install av spandrel albumentations insightface onnx opencv-python segment_anything ultralytics onnxruntime\n",
        "!pip install onnxruntime-gpu -y\n",
        "clear_output()\n",
        "!git clone https://github.com/Isi-dev/ComfyUI\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "clear_output()\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_KJNodes.git\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_KJNodes\n",
        "!pip install -r requirements.txt\n",
        "clear_output()\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/Isi-dev/Practical-RIFE\n",
        "%cd /content/Practical-RIFE\n",
        "!pip install git+https://github.com/rk-exxec/scikit-video.git@numpy_deprecation\n",
        "!mkdir -p /content/Practical-RIFE/train_log\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/IFNet_HDv3.py -O /content/Practical-RIFE/train_log/IFNet_HDv3.py\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/RIFE_HDv3.py -O /content/Practical-RIFE/train_log/RIFE_HDv3.py\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/refine.py -O /content/Practical-RIFE/train_log/refine.py\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/flownet.pkl -O /content/Practical-RIFE/train_log/flownet.pkl\n",
        "clear_output()\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "!apt -y install -qq aria2 ffmpeg\n",
        "clear_output()\n",
        "\n",
        "use480p = True # @param {\"type\":\"boolean\"}\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import gc\n",
        "import sys\n",
        "import random\n",
        "import imageio\n",
        "import subprocess\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, Image as IPImage\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from comfy import model_management\n",
        "\n",
        "from nodes import (\n",
        "    CheckpointLoaderSimple,\n",
        "    CLIPLoader,\n",
        "    CLIPTextEncode,\n",
        "    VAEDecode,\n",
        "    VAELoader,\n",
        "    KSampler,\n",
        "    UNETLoader,\n",
        "    LoadImage,\n",
        "    SaveImage,\n",
        "    CLIPVisionLoader,\n",
        "    CLIPVisionEncode,\n",
        "    LoraLoaderModelOnly,\n",
        "    ImageScale\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
        "from custom_nodes.ComfyUI_KJNodes.nodes.model_optimization_nodes import (\n",
        "    WanVideoTeaCacheKJ,\n",
        "    PathchSageAttentionKJ,\n",
        "    SkipLayerGuidanceWanVideo\n",
        ")\n",
        "\n",
        "from comfy_extras.nodes_model_advanced import ModelSamplingSD3\n",
        "from comfy_extras.nodes_images import SaveAnimatedWEBP\n",
        "from comfy_extras.nodes_video import SaveWEBM\n",
        "from comfy_extras.nodes_wan import WanImageToVideo\n",
        "from comfy_extras.nodes_upscale_model import UpscaleModelLoader\n",
        "\n",
        "\n",
        "def download_with_aria2c(link, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "\n",
        "    filename = link.split(\"/\")[-1]\n",
        "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link} -d {folder} -o {filename}\"\n",
        "\n",
        "    print(\"Executing download command:\")\n",
        "    print(command)\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    get_ipython().system(command)\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "\n",
        "def download_civitai_model(civitai_link, civitai_token, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "    import time\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        model_id = civitai_link.split(\"/models/\")[1].split(\"?\")[0]\n",
        "    except IndexError:\n",
        "        raise ValueError(\"Invalid Civitai URL format. Please use a link like: https://civitai.com/api/download/models/1523247?...\")\n",
        "\n",
        "    civitai_url = f\"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor\"\n",
        "    if civitai_token:\n",
        "        civitai_url += f\"&token={civitai_token}\"\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"model_{timestamp}.safetensors\"\n",
        "\n",
        "    full_path = os.path.join(folder, filename)\n",
        "\n",
        "    download_command = f\"wget --max-redirect=10 --show-progress \\\"{civitai_url}\\\" -O \\\"{full_path}\\\"\"\n",
        "    print(\"Downloading from Civitai...\")\n",
        "\n",
        "    os.system(download_command)\n",
        "\n",
        "    local_path = os.path.join(folder, filename)\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        print(f\"LoRA downloaded successfully: {local_path}\")\n",
        "    else:\n",
        "        print(f\"âŒ LoRA download failed or file is empty: {local_path}\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "def download_lora(link, folder=\"/content/ComfyUI/models/loras\", civitai_token=None):\n",
        "    \"\"\"\n",
        "    Download a model file, automatically detecting if it's a Civitai link or huggingface download.\n",
        "\n",
        "    Args:\n",
        "        link: The download URL (either huggingface or Civitai)\n",
        "        folder: Destination folder for the download\n",
        "        civitai_token: Optional token for Civitai downloads (required if link is from Civitai)\n",
        "\n",
        "    Returns:\n",
        "        The filename of the downloaded model\n",
        "    \"\"\"\n",
        "    if \"civitai.com\" in link.lower():\n",
        "        if not civitai_token:\n",
        "            raise ValueError(\"Civitai token is required for Civitai downloads\")\n",
        "        return download_civitai_model(link, civitai_token, folder)\n",
        "    else:\n",
        "        return download_with_aria2c(link, folder)\n",
        "\n",
        "\n",
        "\n",
        "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Colab-optimized download with aria2c\n",
        "\n",
        "    Args:\n",
        "        url: Download URL\n",
        "        dest_dir: Target directory (will be created if needed)\n",
        "        filename: Optional output filename (defaults to URL filename)\n",
        "        silent: If True, suppresses all output (except errors)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create destination directory\n",
        "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Set filename if not specified\n",
        "        if filename is None:\n",
        "            filename = url.split('/')[-1].split('?')[0]  # Remove URL parameters\n",
        "\n",
        "        # Build command\n",
        "        cmd = [\n",
        "            'aria2c',\n",
        "            '--console-log-level=error',\n",
        "            '-c', '-x', '16', '-s', '16', '-k', '1M',\n",
        "            '-d', dest_dir,\n",
        "            '-o', filename,\n",
        "            url\n",
        "        ]\n",
        "\n",
        "        # Add silent flags if requested\n",
        "        if silent:\n",
        "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
        "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
        "\n",
        "        # Run download\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "\n",
        "        if silent:\n",
        "            print(\"Done!\")\n",
        "        else:\n",
        "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
        "        return filename\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error = e.stderr.strip() or \"Unknown error\"\n",
        "        print(f\"\\nError downloading {filename}: {error}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "model_quant = \"Q4_K_M\" # @param [\"Q4_0\",\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\"]\n",
        "\n",
        "download_loRA_1 = False # @param {type:\"boolean\"}\n",
        "lora_1_download_url = \"Put your loRA here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_2 = False # @param {type:\"boolean\"}\n",
        "lora_2_download_url = \"https://civitai.com/api/download/models/1636239?type=Model&format=SafeTensor\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_3 = False # @param {type:\"boolean\"}\n",
        "lora_3_download_url = \"https://huggingface.co/Remade-AI/Rotate/resolve/main/rotate_20_epochs.safetensors\"# @param {\"type\":\"string\"}\n",
        "\n",
        "token_if_civitai_url = \"Put your civitai token here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "lora_1 = None\n",
        "if download_loRA_1:\n",
        "    lora_1 = download_lora(lora_1_download_url, civitai_token=token_if_civitai_url)\n",
        "# Validate loRA file extension\n",
        "valid_extensions = {'.safetensors', '.ckpt', '.pt', '.pth', '.sft'}\n",
        "if lora_1:\n",
        "    if not any(lora_1.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"âŒ Invalid LoRA format: {lora_1}\")\n",
        "        lora_1 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 1 downloaded succesfully!\")\n",
        "\n",
        "lora_2 = None\n",
        "if download_loRA_2:\n",
        "    lora_2 = download_lora(lora_2_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_2:\n",
        "    if not any(lora_2.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"âŒ Invalid LoRA format: {lora_2}\")\n",
        "        lora_2 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 2 downloaded succesfully!\")\n",
        "\n",
        "lora_3 = None\n",
        "if download_loRA_3:\n",
        "    lora_3 = download_lora(lora_3_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_3:\n",
        "    if not any(lora_3.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"âŒ Invalid LoRA format: {lora_3}\")\n",
        "        lora_3 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 3 downloaded succesfully!\")\n",
        "\n",
        "\n",
        "if use480p:\n",
        "    if model_quant == \"Q4_K_M\":\n",
        "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-480p-Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "    elif model_quant == \"Q5_K_M\":\n",
        "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-480p-Q5_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "    elif model_quant == \"Q6_K\":\n",
        "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-480p-Q6_K.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "    elif model_quant == \"Q4_0\":\n",
        "        dit_model = model_download(\"https://huggingface.co/city96/Wan2.1-I2V-14B-480P-gguf/resolve/main/wan2.1-i2v-14b-480p-Q4_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "    else:\n",
        "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-480p-Q8_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "\n",
        "else:\n",
        "    if model_quant == \"Q4_K_M\":\n",
        "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-720p-Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "    elif model_quant == \"Q5_K_M\":\n",
        "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-720p-Q5_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "    elif model_quant == \"Q6_K\":\n",
        "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-720p-Q6_K.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "    elif model_quant == \"Q4_0\":\n",
        "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-720p-Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "    else:\n",
        "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-720p-Q8_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "\n",
        "\n",
        "clear_output()\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors -d /content/ComfyUI/models/text_encoders -o umt5_xxl_fp8_e4m3fn_scaled.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors -d /content/ComfyUI/models/vae -o wan_2.1_vae.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors -d /content/ComfyUI/models/clip_vision -o clip_vision_h.safetensors\n",
        "clear_output()\n",
        "\n",
        "causvid_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/Wan21_CausVid_14B_T2V_lora_rank32.safetensors\", \"/content/ComfyUI/models/loras\")\n",
        "lightx2v_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank32_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
        "fusionX_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/Wan2.1_I2V_14B_FusionX_LoRA.safetensors\", \"/content/ComfyUI/models/loras\")\n",
        "\n",
        "\n",
        "def upload_file():\n",
        "    \"\"\"Handle file upload (image or video) and return paths.\"\"\"\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "        shutil.move(src_path, dest_path)\n",
        "        paths.append(dest_path)\n",
        "        print(f\"File saved to: {dest_path}\")\n",
        "\n",
        "    return paths[0] if paths else None\n",
        "\n",
        "def extract_frames(video_path, max_frames=None):\n",
        "    \"\"\"Extract frames from video and return as a batch tensor.\"\"\"\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        success, frame = vidcap.read()\n",
        "        if not success or (max_frames and len(frames) >= max_frames):\n",
        "            break\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = torch.from_numpy(frame).float() / 255.0\n",
        "        frames.append(frame)\n",
        "\n",
        "    if not frames:\n",
        "        return None, fps\n",
        "\n",
        "    # Stack frames into a batch tensor: (N, H, W, 3)\n",
        "    batch = torch.stack(frames, dim=0)\n",
        "    # print(f\"Extracted {len(frames)} frames (shape: {batch.shape})\")\n",
        "    return batch, fps\n",
        "\n",
        "\n",
        "def select_every_n_frame_tensor(\n",
        "    frames_tensor: torch.Tensor,\n",
        "    fps: float,\n",
        "    n: int,\n",
        "    skip_first: int = 0,\n",
        "    max_output_frames: int = 0\n",
        "):\n",
        "    if frames_tensor is None or frames_tensor.ndim != 4:\n",
        "        raise ValueError(\"frames_tensor must be a 4D tensor of shape (N, H, W, C)\")\n",
        "    if n < 1:\n",
        "        raise ValueError(\"n must be >= 1\")\n",
        "\n",
        "    total_frames = frames_tensor.shape[0]\n",
        "\n",
        "    if skip_first >= total_frames:\n",
        "        print(\"No frames available after skipping.\")\n",
        "        return None, 0.0\n",
        "\n",
        "    frames_to_use = frames_tensor[skip_first:]\n",
        "\n",
        "    # Select every nth frame\n",
        "    selected_frames = frames_to_use[::n]\n",
        "\n",
        "    # Cap output if needed\n",
        "    if max_output_frames > 0 and selected_frames.shape[0] > max_output_frames:\n",
        "        selected_frames = selected_frames[:max_output_frames]\n",
        "\n",
        "    adjusted_fps = fps / n\n",
        "\n",
        "    if max_output_frames:\n",
        "        print(f\"Frame cap: {max_output_frames} -> Final output: {selected_frames.shape[0]} frames\")\n",
        "    # print(f\"Adjusted FPS: {adjusted_fps:.2f}  -> Final output: {selected_frames.shape[0]} frames\")\n",
        "\n",
        "    return selected_frames, adjusted_fps\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def image_width_height(image):\n",
        "    if image.ndim == 4:\n",
        "        _, height, width, _ = image.shape\n",
        "    elif image.ndim == 3:\n",
        "        height, width, _ = image.shape\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
        "    return width, height\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "    gc.collect()\n",
        "\n",
        "def save_as_mp4(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "    with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_mp4U(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "    frames = []\n",
        "    for i, img in enumerate(images):\n",
        "        try:\n",
        "\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.cpu().numpy()\n",
        "\n",
        "            # print(f\"Frame {i} initial shape: {img.shape}, dtype: {img.dtype}, max: {img.max()}\")  # Debug\n",
        "\n",
        "\n",
        "            if img.max() <= 1.0:\n",
        "                img = (img * 255).astype(np.uint8)\n",
        "            else:\n",
        "                img = img.astype(np.uint8)\n",
        "\n",
        "\n",
        "            if len(img.shape) == 4:  # Batch dimension? (N, C, H, W)\n",
        "                img = img[0]  # Take first image in batch\n",
        "\n",
        "            if len(img.shape) == 3:\n",
        "                if img.shape[0] in (1, 3, 4):  # CHW format\n",
        "                    img = np.transpose(img, (1, 2, 0))\n",
        "                elif img.shape[2] > 4:  # Too many channels\n",
        "                    img = img[:, :, :3]\n",
        "            elif len(img.shape) == 2:\n",
        "                img = np.expand_dims(img, axis=-1)\n",
        "\n",
        "            # print(f\"Frame {i} processed shape: {img.shape}\")  # Debug\n",
        "\n",
        "            # Final validation\n",
        "            if len(img.shape) != 3 or img.shape[2] not in (1, 3, 4):\n",
        "                raise ValueError(f\"Invalid frame shape after processing: {img.shape}\")\n",
        "\n",
        "            frames.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing frame {i}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    try:\n",
        "        with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "            for i, frame in enumerate(frames):\n",
        "                # print(f\"Writing frame {i} with shape: {frame.shape}\")  # Debug\n",
        "                writer.append_data(frame)\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing video: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_webp(images, filename_prefix, fps, quality=90, lossless=False, method=4, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save images as animated WEBP using imageio.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.webp\"\n",
        "\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "\n",
        "    kwargs = {\n",
        "        'fps': int(fps),\n",
        "        'quality': int(quality),\n",
        "        'lossless': bool(lossless),\n",
        "        'method': int(method)\n",
        "    }\n",
        "\n",
        "    with imageio.get_writer(\n",
        "        output_path,\n",
        "        format='WEBP',\n",
        "        mode='I',\n",
        "        **kwargs\n",
        "    ) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_webm(images, filename_prefix, fps, codec=\"vp9\", quality=32, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save images as WEBM using imageio.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.webm\"\n",
        "\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "\n",
        "    kwargs = {\n",
        "        'fps': int(fps),\n",
        "        'quality': int(quality),\n",
        "        'codec': str(codec),\n",
        "        'output_params': ['-crf', str(int(quality))]\n",
        "    }\n",
        "\n",
        "    with imageio.get_writer(\n",
        "        output_path,\n",
        "        format='FFMPEG',\n",
        "        mode='I',\n",
        "        **kwargs\n",
        "    ) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_image(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save single frame as PNG image.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
        "\n",
        "    frame = (image.cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "    Image.fromarray(frame).save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_image2(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save single frame as PNG image.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
        "\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.cpu().numpy()\n",
        "    if image.ndim == 4:  # Batch dimension\n",
        "        image = image[0]\n",
        "    if image.shape[0] == 3:  # CHW to HWC\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "    image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    Image.fromarray(image).save(output_path)\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def upload_image():\n",
        "    \"\"\"Handle image upload in Colab and store in /content/ComfyUI/input/\"\"\"\n",
        "    from google.colab import files\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Move each uploaded file to ComfyUI input directory\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "\n",
        "        shutil.move(src_path, dest_path)\n",
        "        # print(f\"Image saved to: {dest_path}\")\n",
        "        return dest_path\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "output_path =\"\"\n",
        "\n",
        "\n",
        "def generate_video(\n",
        "    image_path: str = None,\n",
        "    LoRA_Strength: float = 1.00,\n",
        "    rel_l1_thresh: float = 0.275,\n",
        "    start_percent: float = 0.1,\n",
        "    end_percent: float = 1.0,\n",
        "    positive_prompt: str = \"a cute anime girl with massive fennec ears and a big fluffy tail wearing a maid outfit turning around\",\n",
        "    negative_prompt: str = \"è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£Žæ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½Žè´¨é‡ï¼ŒJPEGåŽ‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èžåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°\",\n",
        "    width: int = 832,\n",
        "    height: int = 480,\n",
        "    seed: int = 82628696717253,\n",
        "    steps: int = 20,\n",
        "    cfg_scale: float = 1.0,\n",
        "    sampler_name: str = \"uni_pc\",\n",
        "    scheduler: str = \"simple\",\n",
        "    frames: int = 33,\n",
        "    fps: int = 16,\n",
        "    output_format: str = \"mp4\",\n",
        "    overwrite: bool = False,\n",
        "    use_lora: bool = True,\n",
        "    use_lora2: bool = True,\n",
        "    LoRA_Strength2: float = 1.00,\n",
        "    use_lora3: bool = True,\n",
        "    LoRA_Strength3: float = 1.00,\n",
        "    use_causvid: bool = False,\n",
        "    causvid_Strength: float = 0.80,\n",
        "    causvid_steps: int = 4,\n",
        "    use_lightx2v: bool = False,\n",
        "    lightx2v_Strength: float = 0.80,\n",
        "    lightx2v_steps: int = 4,\n",
        "    use_fusionx: bool = False,\n",
        "    fusionx_Strength: float = 0.80,\n",
        "    fusionx_steps: int = 4,\n",
        "    use_sage_attention: bool = True,\n",
        "    enable_flow_shift: bool = True,\n",
        "    shift: float = 8.0\n",
        "):\n",
        "\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        # Initialize nodes\n",
        "        unet_loader = UnetLoaderGGUF()\n",
        "        pathch_sage_attention = PathchSageAttentionKJ()\n",
        "        teacache = WanVideoTeaCacheKJ()\n",
        "        model_sampling = ModelSamplingSD3()\n",
        "        clip_loader = CLIPLoader()\n",
        "        clip_encode_positive = CLIPTextEncode()\n",
        "        clip_encode_negative = CLIPTextEncode()\n",
        "        vae_loader = VAELoader()\n",
        "        clip_vision_loader = CLIPVisionLoader()\n",
        "        clip_vision_encode = CLIPVisionEncode()\n",
        "        load_image = LoadImage()\n",
        "        wan_image_to_video = WanImageToVideo()\n",
        "        ksampler = KSampler()\n",
        "        vae_decode = VAEDecode()\n",
        "        save_webp = SaveAnimatedWEBP()\n",
        "        save_webm = SaveWEBM()\n",
        "        load_lora = LoraLoaderModelOnly()\n",
        "        load_lora2 = LoraLoaderModelOnly()\n",
        "        load_lora3 = LoraLoaderModelOnly()\n",
        "        load_causvid_lora = LoraLoaderModelOnly()\n",
        "        load_lightx2v_lora = LoraLoaderModelOnly()\n",
        "        load_fusionX_lora = LoraLoaderModelOnly()\n",
        "        image_scaler = ImageScale()\n",
        "\n",
        "        print(\"Loading Text_Encoder...\")\n",
        "        clip = clip_loader.load_clip(\"umt5_xxl_fp8_e4m3fn_scaled.safetensors\", \"wan\", \"default\")[0]\n",
        "\n",
        "        positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n",
        "        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        del clip\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        if image_path is None:\n",
        "            print(\"Please upload an image file:\")\n",
        "            image_path = upload_image()\n",
        "        if image_path is None:\n",
        "            print(\"No image uploaded!\")\n",
        "        loaded_image = load_image.load_image(image_path)[0]\n",
        "\n",
        "        width_int, height_int = image_width_height(loaded_image)\n",
        "\n",
        "        if height == 0:\n",
        "            height = int(width * height_int / width_int)\n",
        "\n",
        "        print(f\"Image resolution is {width_int}x{height_int}\")\n",
        "        print(f\"Scaling image to {width}x{height}...\")\n",
        "        loaded_image = image_scaler.upscale(\n",
        "            loaded_image,\n",
        "            \"lanczos\",\n",
        "            width,\n",
        "            height,\n",
        "            \"disabled\"\n",
        "        )[0]\n",
        "\n",
        "        clip_vision = clip_vision_loader.load_clip(\"clip_vision_h.safetensors\")[0]\n",
        "        clip_vision_output = clip_vision_encode.encode(clip_vision, loaded_image, \"none\")[0]\n",
        "\n",
        "        del clip_vision\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(\"Loading VAE...\")\n",
        "        vae = vae_loader.load_vae(\"wan_2.1_vae.safetensors\")[0]\n",
        "\n",
        "        positive_out, negative_out, latent = wan_image_to_video.encode(\n",
        "            positive, negative, vae, width, height, frames, 1, loaded_image, clip_vision_output\n",
        "        )\n",
        "\n",
        "        usedSteps = steps\n",
        "\n",
        "        print(\"Loading Unet Model...\")\n",
        "        model = unet_loader.load_unet(dit_model)[0]\n",
        "\n",
        "        if enable_flow_shift:\n",
        "            model = model_sampling.patch(model, shift)[0]\n",
        "\n",
        "        if use_lora and lora_1 is not None:\n",
        "            print(\"Loading LoRA...\")\n",
        "            model = load_lora.load_lora_model_only(model, lora_1, LoRA_Strength)[0]\n",
        "\n",
        "        if use_lora2 and lora_2 is not None:\n",
        "            print(\"Loading LoRA 2...\")\n",
        "            model = load_lora2.load_lora_model_only(model, lora_2, LoRA_Strength2)[0]\n",
        "\n",
        "        if use_lora3 and lora_3 is not None:\n",
        "            print(\"Loading LoRA 3...\")\n",
        "            model = load_lora3.load_lora_model_only(model, lora_3, LoRA_Strength3)[0]\n",
        "\n",
        "        if use_causvid:\n",
        "            print(\"Loading causvid LoRA...\")\n",
        "            model = load_causvid_lora.load_lora_model_only(model, causvid_lora, causvid_Strength)[0]\n",
        "            usedSteps=causvid_steps\n",
        "\n",
        "        if use_lightx2v:\n",
        "            print(\"Loading lightx2v LoRA...\")\n",
        "            model = load_lightx2v_lora.load_lora_model_only(model, lightx2v_lora, lightx2v_Strength)[0]\n",
        "            usedSteps=lightx2v_steps\n",
        "\n",
        "        if use_fusionx:\n",
        "            print(\"Loading fusionx LoRA...\")\n",
        "            model = load_fusionX_lora.load_lora_model_only(model, fusionX_lora, fusionx_Strength)[0]\n",
        "            usedSteps=fusionx_steps\n",
        "\n",
        "        if use_sage_attention:\n",
        "            model = pathch_sage_attention.patch(model, \"auto\")[0]\n",
        "\n",
        "        if rel_l1_thresh > 0:\n",
        "            print(\"Setting Teacache...\")\n",
        "            if use480p:\n",
        "                model = teacache.patch_teacache(model, rel_l1_thresh, start_percent, end_percent, \"main_device\", \"i2v_480\")[0]\n",
        "            else:\n",
        "                model = teacache.patch_teacache(model, rel_l1_thresh, start_percent, end_percent, \"main_device\", \"i2v_720\")[0]\n",
        "\n",
        "        clear_output()\n",
        "\n",
        "        print(\"Generating video...\")\n",
        "        sampled = ksampler.sample(\n",
        "            model=model,\n",
        "            seed=seed,\n",
        "            steps=usedSteps,\n",
        "            cfg=cfg_scale,\n",
        "            sampler_name=sampler_name,\n",
        "            scheduler=scheduler,\n",
        "            positive=positive_out,\n",
        "            negative=negative_out,\n",
        "            latent_image=latent\n",
        "        )[0]\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        try:\n",
        "            print(\"Decoding latents...\")\n",
        "            decoded = vae_decode.decode(vae, sampled)[0]\n",
        "\n",
        "            del vae\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            global output_path\n",
        "            import datetime\n",
        "            base_name = \"ComfyUI\"\n",
        "            if not overwrite:\n",
        "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                base_name += f\"_{timestamp}\"\n",
        "            if frames == 1:\n",
        "                print(\"Single frame detected - saving as PNG image...\")\n",
        "                output_path = save_as_image(decoded[0], \"ComfyUI\")\n",
        "                # print(f\"Image saved as PNG: {output_path}\")\n",
        "\n",
        "                display(IPImage(filename=output_path))\n",
        "            else:\n",
        "                if output_format.lower() == \"webm\":\n",
        "                    print(\"Saving as WEBM...\")\n",
        "                    output_path = save_as_webm(\n",
        "                        decoded,\n",
        "                        base_name,\n",
        "                        fps=fps,\n",
        "                        codec=\"vp9\",\n",
        "                        quality=10\n",
        "                    )\n",
        "                elif output_format.lower() == \"mp4\":\n",
        "                    print(\"Saving as MP4...\")\n",
        "                    output_path = save_as_mp4(decoded, base_name, fps)\n",
        "\n",
        "                    # output_path = save_as_mp4(decoded, \"ComfyUI\", fps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported output format: {output_format}\")\n",
        "\n",
        "                # print(f\"Video saved as {output_format.upper()}: {output_path}\")\n",
        "\n",
        "                display_video(output_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during decoding/saving: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            clear_memory()\n",
        "\n",
        "\n",
        "\n",
        "def display_video(video_path):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    video_data = open(video_path,'rb').read()\n",
        "\n",
        "    # Determine MIME type based on file extension\n",
        "    if video_path.lower().endswith('.mp4'):\n",
        "        mime_type = \"video/mp4\"\n",
        "    elif video_path.lower().endswith('.webm'):\n",
        "        mime_type = \"video/webm\"\n",
        "    elif video_path.lower().endswith('.webp'):\n",
        "        mime_type = \"image/webp\"\n",
        "    else:\n",
        "        mime_type = \"video/mp4\"  # default\n",
        "\n",
        "    data_url = f\"data:{mime_type};base64,\" + b64encode(video_data).decode()\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=512 controls autoplay loop>\n",
        "        <source src=\"{data_url}\" type=\"{mime_type}\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"âœ… Environment Setup Complete!\")\n",
        "\n",
        "\n",
        "# @markdown ---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @markdown # ðŸ’¥2. Upload Image\n",
        "file_uploaded = upload_image()\n",
        "display_upload = False # @param {type:\"boolean\"}\n",
        "if display_upload:\n",
        "    if file_uploaded.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        display(IPImage(filename=file_uploaded))\n",
        "    else:\n",
        "        print(\"Image format cannnot be displayed.\")\n",
        "# @markdown ---"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zWqdWqkdNxtM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "fa7ea51b-5f6b-4f1a-c977-b72f580465f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c6f73e7b-7fbe-42ac-a787-158199dc157b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c6f73e7b-7fbe-42ac-a787-158199dc157b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1mode.png to 1mode.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @markdown # ðŸ’¥3. Generate Video\n",
        "import time\n",
        "start_time = time.time()\n",
        "# @markdown ### Video Settings\n",
        "positive_prompt = \"The beautiful woman walks forward and smiles as the camera pulls out to reveal more of the scene. She is walking to viewers\" # @param {\"type\":\"string\"}\n",
        "negative_prompt = \"è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£Žæ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½Žè´¨é‡ï¼ŒJPEGåŽ‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èžåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°\" # @param {\"type\":\"string\"}\n",
        "width = 480 # @param {\"type\":\"number\"}\n",
        "height = 854 # @param {\"type\":\"number\"}\n",
        "seed = 2994728291 # @param {\"type\":\"integer\"}\n",
        "steps = 20 # @param {\"type\":\"integer\", \"min\":1, \"max\":100}\n",
        "cfg_scale = 1 # @param {\"type\":\"number\", \"min\":1, \"max\":20}\n",
        "sampler_name = \"lcm\" # @param [\"uni_pc\", \"uni_pc_bh2\", \"ddim\",\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\",\"gradient_estimation\", \"er_sde\", \"seeds_2\", \"seeds_3\"]\n",
        "scheduler = \"simple\" # @param [\"simple\",\"normal\",\"karras\",\"exponential\",\"sgm_uniform\",\"ddim_uniform\",\"beta\",\"linear_quadratic\",\"kl_optimal\"]\n",
        "frames = 81 # @param {\"type\":\"integer\", \"min\":1, \"max\":120}\n",
        "# fps = 16 # @param {\"type\":\"integer\", \"min\":1, \"max\":60}\n",
        "# output_format = \"mp4\" # @param [\"mp4\", \"webm\"]\n",
        "fps = 16\n",
        "output_format = \"mp4\"\n",
        "overwrite_previous_video = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ### Model Configuration\n",
        "use_sage_attention = True # @param {type:\"boolean\"}\n",
        "# use_sage_attention = True\n",
        "use_flow_shift = True # @param {type:\"boolean\"}\n",
        "flow_shift = 3 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":100.0,\"step\":0.01}\n",
        "\n",
        "\n",
        "# @markdown ### Wan2.1 Based Models LoRA Configuration\n",
        "use_causvid = False # @param {type:\"boolean\"}\n",
        "causvid_Strength = 0.8 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":0.01}\n",
        "causvid_steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":20}\n",
        "use_lightx2v = True # @param {type:\"boolean\"}\n",
        "lightx2v_Strength = 0.5 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":0.01}\n",
        "lightx2v_steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":20}\n",
        "use_fusionx = True # @param {type:\"boolean\"}\n",
        "fusionx_Strength = 0.5 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":0.01}\n",
        "fusionx_steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":20}\n",
        "\n",
        "\n",
        "# @markdown ### LoRA Configuration\n",
        "use_lora = False # @param {type:\"boolean\"}\n",
        "LoRA_Strength = 1.0 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":0.01}\n",
        "use_lora2 = False # @param {type:\"boolean\"}\n",
        "LoRA_Strength2 = 1.0 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":0.01}\n",
        "use_lora3 = False # @param {type:\"boolean\"}\n",
        "LoRA_Strength3 = 1.0 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":0.01}\n",
        "\n",
        "# @markdown ### Teacache Settings\n",
        "rel_l1_thresh = 0 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":10,\"step\":0.001}\n",
        "start_percent = 0.2 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":1.0,\"step\":0.01}\n",
        "end_percent = 1.0 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":1.0,\"step\":0.01}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "import random\n",
        "seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n",
        "print(f\"Using seed: {seed}\")\n",
        "\n",
        "# with torch.inference_mode():\n",
        "generate_video(\n",
        "    image_path=file_uploaded,\n",
        "    LoRA_Strength=LoRA_Strength,\n",
        "    rel_l1_thresh=rel_l1_thresh,\n",
        "    start_percent=start_percent,\n",
        "    end_percent = end_percent,\n",
        "    positive_prompt=positive_prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=width,\n",
        "    height=height,\n",
        "    seed=seed,\n",
        "    steps=steps,\n",
        "    cfg_scale=cfg_scale,\n",
        "    sampler_name=sampler_name,\n",
        "    scheduler=scheduler,\n",
        "    frames=frames,\n",
        "    fps=fps,\n",
        "    output_format=output_format,\n",
        "    overwrite=overwrite_previous_video,\n",
        "    use_lora = use_lora,\n",
        "    use_lora2=use_lora2,\n",
        "    LoRA_Strength2=LoRA_Strength2,\n",
        "    use_lora3=use_lora3,\n",
        "    LoRA_Strength3=LoRA_Strength3,\n",
        "    use_causvid=use_causvid,\n",
        "    causvid_Strength=causvid_Strength,\n",
        "    causvid_steps=causvid_steps,\n",
        "    use_lightx2v=use_lightx2v,\n",
        "    lightx2v_Strength=lightx2v_Strength,\n",
        "    lightx2v_steps=lightx2v_steps,\n",
        "    use_fusionx=use_fusionx,\n",
        "    fusionx_Strength=fusionx_Strength,\n",
        "    fusionx_steps=fusionx_steps,\n",
        "    use_sage_attention = use_sage_attention,\n",
        "    enable_flow_shift = use_flow_shift,\n",
        "    shift = flow_shift\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "mins, secs = divmod(duration, 60)\n",
        "print(f\"Seed: {seed}\")\n",
        "print(f\"âœ… Generation completed in {int(mins)} min {secs:.2f} sec\")\n",
        "\n",
        "clear_memory()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fcsJjujta1K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @markdown # ðŸ’¥4. Apply Frame Interpolation\n",
        "# interpolate_optional_video=False # @param {type:\"boolean\"}\n",
        "\n",
        "# if interpolate_optional_video:\n",
        "#     try:\n",
        "#         output_path = oIoutput_path\n",
        "#     except NameError:\n",
        "#         pass\n",
        "\n",
        "\n",
        "import glob\n",
        "from IPython.display import Video as outVid\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "FRAME_MULTIPLIER = 2 # @param {\"type\":\"number\"}\n",
        "vid_fps = 30 # @param {\"type\":\"number\"}\n",
        "crf_value = 17 # @param {\"type\":\"slider\",\"min\":0,\"max\":51,\"step\":1}\n",
        "\n",
        "print(f\"Converting video to {vid_fps} fps...\")\n",
        "\n",
        "%cd /content/Practical-RIFE\n",
        "\n",
        "# Suppress ALSA errors\n",
        "os.environ[\"XDG_RUNTIME_DIR\"] = \"/tmp\"\n",
        "os.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"\n",
        "\n",
        "# Disable warnings from ffmpeg about missing audio\n",
        "os.environ[\"PYGAME_HIDE_SUPPORT_PROMPT\"] = \"1\"\n",
        "os.environ[\"FFMPEG_LOGLEVEL\"] = \"quiet\"\n",
        "\n",
        "!python3 inference_video.py --multi={FRAME_MULTIPLIER} --fps={vid_fps} --video={output_path} --scale={1}\n",
        "video_folder = \"/content/ComfyUI/output/\"\n",
        "\n",
        "# Find the latest MP4 file\n",
        "video_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
        "\n",
        "if video_files:\n",
        "    latest_video = max(video_files, key=os.path.getctime)\n",
        "    # !ffmpeg -i \"{latest_video}\" -vcodec libx264 -crf 18 -preset fast output_converted.mp4 -loglevel error -y\n",
        "    !ffmpeg -i \"{latest_video}\" -vcodec libx264 -crf {crf_value} -preset fast output_converted.mp4 -loglevel error -y\n",
        "\n",
        "    print(f\"Displaying video: {latest_video}\")\n",
        "    # display(outVid(\"output_converted.mp4\", embed=True))\n",
        "    display_video(\"output_converted.mp4\")\n",
        "    # displayVid(outVid(latest_video, embed=True))\n",
        "else:\n",
        "    print(\"âŒ No video found in output/\")\n",
        "\n",
        "del video_files\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "mins, secs = divmod(duration, 60)\n",
        "print(f\"âœ… Frame Interpolation completed in {int(mins)} min {secs:.2f} sec\")\n",
        "\n",
        "clear_memory()\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "# @markdown ---"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lTCG6eiN68Ib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}